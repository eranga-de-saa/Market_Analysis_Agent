{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "45c2e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Literal\n",
    "from enum import Enum\n",
    "from datetime import date\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "import re\n",
    "\n",
    "class AnalysisIntent(str, Enum):\n",
    "    PERFORMANCE = \"performance\"\n",
    "    RISK = \"risk\"\n",
    "    VOLATILITY = \"volatility\"\n",
    "    LIQUIDITY = \"liquidity\"\n",
    "    CORRELATION = \"correlation\"\n",
    "    VALUATION = \"valuation\"\n",
    "    EVENT_IMPACT = \"event_impact\"\n",
    "    REGIME_DETECTION = \"regime_detection\"\n",
    "    EXPLORATORY = \"exploratory\"\n",
    "\n",
    "class MarketType(str, Enum):\n",
    "    EQUITIES = \"equities\"\n",
    "    CRYPTO = \"crypto\"\n",
    "    RATES = \"rates\"\n",
    "    FX = \"fx\"\n",
    "    COMMODITIES = \"commodities\"\n",
    "    MIXED = \"mixed\"\n",
    "\n",
    "\n",
    "class Metric(str, Enum):\n",
    "    TOTAL_RETURN = \"total_return\"\n",
    "    CAGR = \"cagr\"\n",
    "    SHARPE_RATIO = \"sharpe_ratio\"\n",
    "    MAX_DRAWDOWN = \"max_drawdown\"\n",
    "\n",
    "    REALIZED_VOLATILITY = \"realized_volatility\"\n",
    "    ROLLING_VOLATILITY = \"rolling_volatility\"\n",
    "\n",
    "    ROLLING_CORRELATION = \"rolling_correlation\"\n",
    "    BETA = \"beta\"\n",
    "\n",
    "    VOLUME = \"volume\"\n",
    "    AMIHUD_ILLIQUIDITY = \"amihud_illiquidity\"\n",
    "\n",
    "    REGIME_SWITCHING = \"regime_switching\"\n",
    "\n",
    "\n",
    "class Universe(BaseModel):\n",
    "    assets: List[str] = Field(min_length=1)\n",
    "    benchmark: Optional[str] = None\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    def normalize_symbols(cls, v):\n",
    "        def normalize(s: str) -> str:\n",
    "            m = re.search(r\"\\(([A-Z\\.]+)\\)\", s)\n",
    "            return m.group(1) if m else s.upper().strip()\n",
    "\n",
    "        v[\"assets\"] = [normalize(a) for a in v.get(\"assets\", [])]\n",
    "\n",
    "        if v.get(\"benchmark\"):\n",
    "            v[\"benchmark\"] = normalize(v[\"benchmark\"])\n",
    "\n",
    "        return v\n",
    "    \n",
    "class TimeWindow(BaseModel):\n",
    "    start: Optional[date] = None\n",
    "    end: Optional[date] = None\n",
    "    lookback_years: Optional[int] = Field(gt=0)\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_time_window(self):\n",
    "        explicit = self.start is not None or self.end is not None\n",
    "        relative = self.lookback_years is not None\n",
    "\n",
    "        if explicit and relative:\n",
    "            raise ValueError(\"Use either start/end or lookback_years, not both\")\n",
    "\n",
    "        if not explicit and not relative:\n",
    "            raise ValueError(\"Time window must be specified\")\n",
    "\n",
    "        if explicit and not (self.start and self.end):\n",
    "            raise ValueError(\"Both start and end must be provided\")\n",
    "\n",
    "        return self\n",
    "\n",
    "class ConfidenceRequirements(BaseModel):\n",
    "    confidence_level: float = Field(default=0.95, gt=0.5, lt=1.0)\n",
    "    alpha: float = Field(default=0.05, gt=0.0, lt=0.2)\n",
    "    require_confidence_intervals: bool = True\n",
    "\n",
    "\n",
    "\n",
    "class AnalysisPlan(BaseModel):\n",
    "    topic: str = Field(min_length=10)\n",
    "\n",
    "    intent: AnalysisIntent\n",
    "    market: MarketType\n",
    "\n",
    "    universe: Universe\n",
    "    metrics: List[Metric] = Field(min_length=1)\n",
    "\n",
    "    time_window: TimeWindow\n",
    "    frequency: Literal[\"daily\", \"weekly\", \"monthly\"]\n",
    "\n",
    "    confidence_requirements: Optional[ConfidenceRequirements] = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "279718f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state.py\n",
    "from typing import Any, Dict, List, Optional\n",
    "from typing_extensions import TypedDict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class MarketAnalysisState(TypedDict):\n",
    "    # User input\n",
    "    user_prompt: str\n",
    "\n",
    "    # Planning output\n",
    "    analysis_plan: Optional[AnalysisPlan]\n",
    "\n",
    "    # Market data\n",
    "    raw_market_data: Optional[Any]\n",
    "    cleaned_market_data: Optional[Any]\n",
    "\n",
    "    # Statistics\n",
    "    computed_metrics: Optional[Dict[str, Any]]\n",
    "\n",
    "    # Qualitative context\n",
    "    external_context: List[str]\n",
    "\n",
    "    # Validation\n",
    "    validation_results: Optional[Dict[str, Any]]\n",
    "    sufficient_data: Optional[bool]\n",
    "\n",
    "    # Control flow\n",
    "    iteration_count: int\n",
    "\n",
    "    # Outputs\n",
    "    final_report: Optional[Dict[str, Any]]\n",
    "    pdf_path: Optional[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e52c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrypoint.py\n",
    "#from state import MarketAnalysisState\n",
    "\n",
    "\n",
    "def initialize_state(user_prompt: str) -> dict:\n",
    "    \"\"\"\n",
    "    Entry point for the entire graph.\n",
    "    Initializes shared state with safe defaults.\n",
    "    \"\"\"\n",
    "\n",
    "    return {\n",
    "        \"user_prompt\": user_prompt\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21f62aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal, Optional, TypedDict\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ------------------------\n",
    "# 1. LLM INITIALIZATION\n",
    "# ------------------------\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 3. STRUCTURED LLM\n",
    "# ------------------------\n",
    "planner_llm = llm.with_structured_output(AnalysisPlan)\n",
    "\n",
    "# ------------------------\n",
    "# 5. PLANNER NODE\n",
    "# ------------------------\n",
    "def analysis_planner_node(state: MarketAnalysisState) -> dict:\n",
    "    plan = planner_llm.invoke(\n",
    "        f\"\"\"\n",
    "        You are a financial analysis planner.\n",
    "        Produce a valid AnalysisPlan.\n",
    "\n",
    "        User prompt:\n",
    "        {state[\"user_prompt\"]}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"analysis_plan\": plan\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7d5a7ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for AnalysisPlan\ntime_window\n  Value error, Use either start/end or lookback_years, not both [type=value_error, input_value={'start': '2016-01-01', '...e, 'lookback_years': 10}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 35\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 6. TEST RUN\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[1;32m     30\u001b[0m initial_state: MarketAnalysisState \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyze how US mega-cap tech stocks reacted to rising interest rates\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis_plan\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     33\u001b[0m }\n\u001b[0;32m---> 35\u001b[0m final_state \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFINAL STATE\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_state)\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/langgraph/pregel/main.py:3085\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3082\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3083\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 3085\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   3086\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3087\u001b[0m     config,\n\u001b[1;32m   3088\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   3089\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3090\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3091\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[1;32m   3092\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[1;32m   3093\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   3094\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   3095\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   3096\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[1;32m   3097\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3098\u001b[0m ):\n\u001b[1;32m   3099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/langgraph/pregel/main.py:2674\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2673\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2674\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2675\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2676\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2677\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2678\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2679\u001b[0m ):\n\u001b[1;32m   2680\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2682\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2683\u001b[0m     )\n\u001b[1;32m   2684\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/langgraph/pregel/_runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/langgraph/_internal/_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/langgraph/_internal/_runnable.py:401\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[56], line 23\u001b[0m, in \u001b[0;36manalysis_planner_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21manalysis_planner_node\u001b[39m(state: MarketAnalysisState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m---> 23\u001b[0m     plan \u001b[38;5;241m=\u001b[39m \u001b[43mplanner_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;43m        You are a financial analysis planner.\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;43m        Produce a valid AnalysisPlan.\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;43m        User prompt:\u001b[39;49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_prompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstate,\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis_plan\u001b[39m\u001b[38;5;124m\"\u001b[39m: plan\n\u001b[1;32m     36\u001b[0m     }\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/langchain_core/runnables/base.py:3252\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m   3251\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3252\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3253\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3254\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config)\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/langchain_core/runnables/base.py:5720\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5713\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5714\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5715\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5718\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5719\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5721\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5722\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5723\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5724\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:395\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    392\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 395\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    405\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:1025\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1023\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1024\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:842\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    841\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 842\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m         )\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    850\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:1091\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1091\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1095\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/langchain_openai/chat_models/base.py:1213\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_response\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1212\u001b[0m         e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mhttp_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1218\u001b[0m ):\n\u001b[1;32m   1219\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/langchain_openai/chat_models/base.py:1185\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mwith_raw_response\u001b[38;5;241m.\u001b[39mparse(\n\u001b[1;32m   1182\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload\n\u001b[1;32m   1183\u001b[0m         )\n\u001b[1;32m   1184\u001b[0m     )\n\u001b[0;32m-> 1185\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mraw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1187\u001b[0m     _handle_openai_bad_request(e)\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/openai/_legacy_response.py:139\u001b[0m, in \u001b[0;36mLegacyAPIResponse.parse\u001b[0;34m(self, to)\u001b[0m\n\u001b[1;32m    137\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse(to\u001b[38;5;241m=\u001b[39mto)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_given(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\u001b[38;5;241m.\u001b[39mpost_parser):\n\u001b[0;32m--> 139\u001b[0m     parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed, BaseModel):\n\u001b[1;32m    142\u001b[0m     add_request_id(parsed, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_id)\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/openai/resources/chat/completions/completions.py:178\u001b[0m, in \u001b[0;36mCompletions.parse.<locals>.parser\u001b[0;34m(raw_completion)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparser\u001b[39m(raw_completion: ChatCompletion) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedChatCompletion[ResponseFormatT]:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_completion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_tools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_completion_tools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/openai/lib/_parsing/_completions.py:146\u001b[0m, in \u001b[0;36mparse_chat_completion\u001b[0;34m(response_format, input_tools, chat_completion)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m                 tool_calls\u001b[38;5;241m.\u001b[39mappend(tool_call)\n\u001b[1;32m    139\u001b[0m     choices\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    140\u001b[0m         construct_type_unchecked(\n\u001b[1;32m    141\u001b[0m             type_\u001b[38;5;241m=\u001b[39mcast(Any, ParsedChoice)[solve_response_format_t(response_format)],\n\u001b[1;32m    142\u001b[0m             value\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    143\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchoice\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m    144\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    145\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmessage\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[0;32m--> 146\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmaybe_parse_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    150\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_calls \u001b[38;5;28;01mif\u001b[39;00m tool_calls \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    151\u001b[0m                 },\n\u001b[1;32m    152\u001b[0m             },\n\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    157\u001b[0m     ParsedChatCompletion[ResponseFormatT],\n\u001b[1;32m    158\u001b[0m     construct_type_unchecked(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m     ),\n\u001b[1;32m    165\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/openai/lib/_parsing/_completions.py:199\u001b[0m, in \u001b[0;36mmaybe_parse_content\u001b[0;34m(response_format, message)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmaybe_parse_content\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    195\u001b[0m     response_format: \u001b[38;5;28mtype\u001b[39m[ResponseFormatT] \u001b[38;5;241m|\u001b[39m ResponseFormatParam \u001b[38;5;241m|\u001b[39m Omit,\n\u001b[1;32m    196\u001b[0m     message: ChatCompletionMessage \u001b[38;5;241m|\u001b[39m ParsedChatCompletionMessage[\u001b[38;5;28mobject\u001b[39m],\n\u001b[1;32m    197\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseFormatT \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_rich_response_format(response_format) \u001b[38;5;129;01mand\u001b[39;00m message\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message\u001b[38;5;241m.\u001b[39mrefusal:\n\u001b[0;32m--> 199\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/openai/lib/_parsing/_completions.py:262\u001b[0m, in \u001b[0;36m_parse_content\u001b[0;34m(response_format, content)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_parse_content\u001b[39m(response_format: \u001b[38;5;28mtype\u001b[39m[ResponseFormatT], content: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseFormatT:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_basemodel_type(response_format):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseFormatT, \u001b[43mmodel_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dataclass_like_type(response_format):\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m PYDANTIC_V1:\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/openai/_compat.py:171\u001b[0m, in \u001b[0;36mmodel_parse_json\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PYDANTIC_V1:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparse_raw(data)  \u001b[38;5;66;03m# pyright: ignore[reportDeprecated]\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/pydantic/main.py:766\u001b[0m, in \u001b[0;36mBaseModel.model_validate_json\u001b[0;34m(cls, json_data, strict, extra, context, by_alias, by_name)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[1;32m    762\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    763\u001b[0m         code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidate-by-alias-and-name-false\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    764\u001b[0m     )\n\u001b[0;32m--> 766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_name\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for AnalysisPlan\ntime_window\n  Value error, Use either start/end or lookback_years, not both [type=value_error, input_value={'start': '2016-01-01', '...e, 'lookback_years': 10}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error"
     ]
    }
   ],
   "source": [
    "from typing import Optional, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# =========================\n",
    "# 5. GRAPH\n",
    "# =========================\n",
    "graph = StateGraph(MarketAnalysisState)\n",
    "\n",
    "def terminal_node(state: MarketAnalysisState) -> MarketAnalysisState:\n",
    "    return state\n",
    "\n",
    "\n",
    "graph.add_node(\"planner\", analysis_planner_node)\n",
    "graph.add_node(\"terminal\", terminal_node)\n",
    "\n",
    "graph.set_entry_point(\"planner\")\n",
    "graph.add_edge(\"planner\", \"terminal\")\n",
    "graph.add_edge(\"terminal\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6. TEST RUN\n",
    "# =========================\n",
    "initial_state: MarketAnalysisState = {\n",
    "    \"user_prompt\": \"Analyze how US mega-cap tech stocks reacted to rising interest rates\",\n",
    "    \"analysis_plan\": None\n",
    "}\n",
    "\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "print(\"\\nFINAL STATE\\n\")\n",
    "print(final_state)\n",
    "print(\"\\nANALYSIS PLAN\\n\")\n",
    "print(final_state[\"analysis_plan\"].model_dump())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0162b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools/maket_data.py\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def _flatten_yfinance_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flatten yfinance MultiIndex columns to single level.\n",
    "    Keeps only price field names (Open, Close, etc).\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # Drop the ticker level\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_market_data(\n",
    "    symbols: List[str],\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    frequency: str\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetch OHLCV data for given symbols.\n",
    "    Returns a dict: {symbol: DataFrame}\n",
    "    \"\"\"\n",
    "\n",
    "    interval_map = {\n",
    "        \"daily\": \"1d\",\n",
    "        \"weekly\": \"1wk\",\n",
    "        \"monthly\": \"1mo\"\n",
    "    }\n",
    "\n",
    "    interval = interval_map[frequency]\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for symbol in symbols:\n",
    "        df = yf.download(\n",
    "            symbol,\n",
    "            start=start_date,\n",
    "            end=end_date,\n",
    "            interval=interval,\n",
    "            auto_adjust=False,\n",
    "            progress=False\n",
    "        )\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"No data returned for {symbol}\")\n",
    "        \n",
    "        df = _flatten_yfinance_columns(df)\n",
    "\n",
    "        # Ensure standard column order if present\n",
    "        preferred_order = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]\n",
    "        df = df[[c for c in preferred_order if c in df.columns]]\n",
    "\n",
    "        data[symbol] = df\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "292e2e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/time.py.  This will evolve later. For now, it is enough.\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Tuple\n",
    "\n",
    "# from analysis_plan import TimeWindow  # wherever your model lives\n",
    "\n",
    "\n",
    "def resolve_time_window(time_window: TimeWindow) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Resolve a structured TimeWindow into ISO start and end dates.\n",
    "    \"\"\"\n",
    "\n",
    "    end = datetime.today()\n",
    "\n",
    "    # Relative window\n",
    "    if time_window.lookback_years is not None:\n",
    "        start = end - timedelta(days=365 * time_window.lookback_years)\n",
    "        return start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Explicit window\n",
    "    if time_window.start and time_window.end:\n",
    "        return (\n",
    "            time_window.start.isoformat(),\n",
    "            time_window.end.isoformat(),\n",
    "        )\n",
    "\n",
    "    # This should never happen due to model validation\n",
    "    raise ValueError(\"Invalid TimeWindow state\")\n",
    "\n",
    "\n",
    "# from analysis_plan import TimeWindow\n",
    "\n",
    "\n",
    "def format_time_window_for_context(time_window: TimeWindow) -> str:\n",
    "    \"\"\"\n",
    "    Convert TimeWindow into human-readable phrasing for search queries.\n",
    "    \"\"\"\n",
    "\n",
    "    if time_window.lookback_years is not None:\n",
    "        return f\"last {time_window.lookback_years} years\"\n",
    "\n",
    "    if time_window.start and time_window.end:\n",
    "        return f\"from {time_window.start.isoformat()} to {time_window.end.isoformat()}\"\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "71de830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes/market_data_collection.py\n",
    "# \n",
    "# from state import MarketAnalysisState\n",
    "# from tools.market_data import fetch_market_data\n",
    "# from utils.time import resolve_time_window\n",
    "\n",
    "\n",
    "def market_data_collection_node(state: MarketAnalysisState) -> dict:\n",
    "    \"\"\"\n",
    "    Collects raw market time series data based on AnalysisPlan.\n",
    "    Safe for re-execution and looping.\n",
    "    \"\"\"\n",
    "\n",
    "    plan = state[\"analysis_plan\"]\n",
    "\n",
    "    if plan is None:\n",
    "        raise ValueError(\"analysis_plan is required before data collection\")\n",
    "\n",
    "    # Resolve structured time window\n",
    "    start_date, end_date = resolve_time_window(plan.time_window)\n",
    "\n",
    "    # Extract assets only (benchmarks handled separately later)\n",
    "    symbols = plan.universe.assets\n",
    "\n",
    "    raw_data = fetch_market_data(\n",
    "        symbols=symbols,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        frequency=plan.frequency\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"raw_market_data\": raw_data\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9f1a08b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['AAPL', 'MSFT'])\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# from state import MarketAnalysisState\n",
    "# from entrypoint import initialize_state\n",
    "# from planner import analysis_planner_node\n",
    "# from nodes.market_data_collection import market_data_collection_node\n",
    "\n",
    "\n",
    "graph = StateGraph(MarketAnalysisState)\n",
    "\n",
    "graph.add_node(\"planner\", analysis_planner_node)\n",
    "graph.add_node(\"data\", market_data_collection_node)\n",
    "\n",
    "graph.set_entry_point(\"planner\")\n",
    "graph.add_edge(\"planner\", \"data\")\n",
    "graph.add_edge(\"data\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "state = initialize_state(\n",
    "    \"Analyze volatility of Apple and Microsoft over the last 3 years\"\n",
    ")\n",
    "\n",
    "final_state = app.invoke(state)\n",
    "\n",
    "print(final_state[\"raw_market_data\"].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "df87d69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Price</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-02-13</th>\n",
       "      <td>150.949997</td>\n",
       "      <td>154.259995</td>\n",
       "      <td>150.919998</td>\n",
       "      <td>153.850006</td>\n",
       "      <td>151.653015</td>\n",
       "      <td>62199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-14</th>\n",
       "      <td>152.119995</td>\n",
       "      <td>153.770004</td>\n",
       "      <td>150.860001</td>\n",
       "      <td>153.199997</td>\n",
       "      <td>151.012299</td>\n",
       "      <td>61707600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-15</th>\n",
       "      <td>153.110001</td>\n",
       "      <td>155.500000</td>\n",
       "      <td>152.880005</td>\n",
       "      <td>155.330002</td>\n",
       "      <td>153.111862</td>\n",
       "      <td>65573800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-16</th>\n",
       "      <td>153.509995</td>\n",
       "      <td>156.330002</td>\n",
       "      <td>153.350006</td>\n",
       "      <td>153.710007</td>\n",
       "      <td>151.514999</td>\n",
       "      <td>68167900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-17</th>\n",
       "      <td>152.350006</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>150.850006</td>\n",
       "      <td>152.550003</td>\n",
       "      <td>150.371536</td>\n",
       "      <td>59144100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price             Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2023-02-13  150.949997  154.259995  150.919998  153.850006  151.653015   \n",
       "2023-02-14  152.119995  153.770004  150.860001  153.199997  151.012299   \n",
       "2023-02-15  153.110001  155.500000  152.880005  155.330002  153.111862   \n",
       "2023-02-16  153.509995  156.330002  153.350006  153.710007  151.514999   \n",
       "2023-02-17  152.350006  153.000000  150.850006  152.550003  150.371536   \n",
       "\n",
       "Price         Volume  \n",
       "Date                  \n",
       "2023-02-13  62199000  \n",
       "2023-02-14  61707600  \n",
       "2023-02-15  65573800  \n",
       "2023-02-16  68167900  \n",
       "2023-02-17  59144100  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state[\"raw_market_data\"][\"AAPL\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "34c9b245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'], dtype='object', name='Price')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state[\"raw_market_data\"][\"MSFT\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28f24715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools/context_research.py\n",
    "from ddgs import DDGS\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "def generate_symbol_topic(\n",
    "    global_topic: str,\n",
    "    symbol: str,\n",
    "    market: str\n",
    ") -> str:\n",
    "    response = llm.invoke(\n",
    "        f\"\"\"\n",
    "        You are refining a financial research topic.\n",
    "\n",
    "        Global topic:\n",
    "        \"{global_topic}\"\n",
    "\n",
    "        Asset:\n",
    "        {symbol}\n",
    "\n",
    "        Market:\n",
    "        {market}\n",
    "\n",
    "        Produce a single, focused research topic specific to this asset.\n",
    "        Do not introduce new themes.\n",
    "        One sentence only.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    return response.content.strip()\n",
    "\n",
    "\n",
    "def research_market_context_for_symbol(\n",
    "    topic: str,\n",
    "    symbol: str,\n",
    "    time_phrase: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Search and summarize qualitative context for a single symbol.\n",
    "    \"\"\"\n",
    "\n",
    "    query = \" \".join([symbol, topic, time_phrase])\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with DDGS() as ddgs:\n",
    "        for r in ddgs.text(query, max_results=5):\n",
    "            if r.get(\"body\"):\n",
    "                results.append(r[\"body\"])\n",
    "\n",
    "    if not results:\n",
    "        return \"No significant qualitative market context found.\"\n",
    "\n",
    "    joined_text = \"\\n\".join(results)\n",
    "\n",
    "    summary = llm.invoke(\n",
    "        f\"\"\"\n",
    "        You are a financial analyst.\n",
    "\n",
    "        Summarize the following information strictly in relation to:\n",
    "        \"{topic}\"\n",
    "\n",
    "        Focus on drivers, risks, and regime-level factors.\n",
    "        Avoid generic macro commentary.\n",
    "\n",
    "        Source material:\n",
    "        {joined_text}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    return summary.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1e720b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nodes/context_research.py\n",
    "# from state import MarketAnalysisState\n",
    "# from tools.context_research import (\n",
    "#     generate_symbol_topic,\n",
    "#     research_market_context_for_symbol\n",
    "# )\n",
    "# from utils.context_time import format_time_window_for_context\n",
    "\n",
    "\n",
    "def context_research_node(state: MarketAnalysisState) -> dict:\n",
    "    \"\"\"\n",
    "    Collects qualitative market context per symbol\n",
    "    and appends results to external_context.\n",
    "    \"\"\"\n",
    "\n",
    "    plan = state[\"analysis_plan\"]\n",
    "\n",
    "    if plan is None:\n",
    "        raise ValueError(\"analysis_plan is required before context research\")\n",
    "\n",
    "    time_phrase = format_time_window_for_context(plan.time_window)\n",
    "\n",
    "    new_entries = []\n",
    "\n",
    "    for symbol in plan.universe.assets:\n",
    "        symbol_topic = generate_symbol_topic(\n",
    "            global_topic=plan.topic,\n",
    "            symbol=symbol,\n",
    "            market=plan.market.value\n",
    "        )\n",
    "\n",
    "        summary = research_market_context_for_symbol(\n",
    "            topic=symbol_topic,\n",
    "            symbol=symbol,\n",
    "            time_phrase=time_phrase\n",
    "        )\n",
    "\n",
    "        new_entries.append(\n",
    "            {\n",
    "                \"symbol\": symbol,\n",
    "                \"topic\": symbol_topic,\n",
    "                \"summary\": summary\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"external_context\": new_entries\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cbd8aaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['AAPL', 'MSFT'])\n",
      "[{'symbol': 'AAPL', 'topic': 'Quantitatively analyze the volatility of Apple Inc. (AAPL) stock over the last three years by applying realized-volatility measures and GARCH-family models to characterize persistence, regime changes, and reaction to major market events.', 'summary': 'Limitations of the provided material\\n- The source material does not include actual realized-volatility figures, GARCH-estimated parameters, or any daily price series for Apple (AAPL) over the last three years.\\n- It only provides: (i) a 52-week average Apple price of 182.81, and (ii) a Q3 earnings snapshot: revenue up 10% YoY to $94B. There is no explicit volatility data or event-dated volatility signals.\\n\\nWhat a quantitative analysis would entail (methodology and expected outputs)\\n- Realized-volatility (RV)\\n  - Compute daily log returns r_t = ln(P_t/P_{t-1}) over roughly the past three years (about 750780 trading days).\\n  - Daily RV: sum of squared returns over a moving window; annualize as RV_y = sqrt(252 * sum(r_t^2) / window).\\n  - Examine time-varying RV levels, RV clustering, and RV around major events (e.g., earnings releases).\\n- GARCH-family models (persistence and regime signals)\\n  - Fit a standard GARCH(1,1) to daily returns to obtain omega, alpha, beta; interpret persistence as alpha + beta.\\n  - Extend to EGARCH/TGARCH to capture leverage effects and asymmetric responses to positive vs. negative returns.\\n  - Use rolling windows or structural breaks to assess regime persistence shifts; optionally apply Markov-switching GARCH to identify low-volatility vs high-volatility regimes and transition probabilities.\\n  - Map regime indicators to event windows (earnings announcements, guidance changes) to gauge reaction strength and persistence.\\n- regime-change interpretation\\n  - High-persistence regimes (alpha + beta near or above ~0.95) imply protracted volatility spells; low-persistence regimes imply quicker mean-reversion.\\n  - Transitions between regimes often align with material news/events or macro-stress periods, yielding abrupt volatility spikes that may persist if the news alters the perceived risk environment.\\n\\nApple-specific drivers and regime-level factors (as inferable from the provided data)\\n- Earnings/news catalysts\\n  - The Q3 snapshot shows revenue up 10% YoY to $94B, which signals positive top-line momentum. In a volatility framework, such earnings-weighted events typically act as catalysts that can trigger a short-lived spike in implied and realized volatility around release date, followed by potential regime shifts if the market revises margins, guidance, or long-run growth expectations.\\n  - If the revenue surprise is accompanied by favorable guidance, the low-to-moderate volatility regime could be reinforced; if margins or durability of growth are questioned, volatility could transition into a higher-persistence regime.\\n- Price level context\\n  - A 52-week average price of 182.81 provides a mid-range reference point for assessing drift versus dispersion in returns. While not a volatility measure itself, it helps contextualize whether observed RV is dominated by large daily moves around events or by longer-run drift.\\n- Regime-persistence expectations\\n  - Absent explicit numerical GARCH parameters in the source, one would test for persistence by estimating alpha + beta. If Apple experiences prolonged volatility episodes around earnings cycles or during sector-wide stress, persistence could be high, indicating that shocks carry into subsequent weeks/months.\\n- Regime-structure and event windows\\n  - Market-relevant events (earnings, product-cycle news, guidance revisions) are natural candidates for regime changes in a GARCH/Markov-switching framework. A two-state model could reveal:\\n    - State 1: low-volatility regime with modest RV, longer inter-event stability.\\n    - State 2: high-volatility regime with elevated RV, more pronounced reaction to earnings and other news.\\n  - Transition probabilities would quantify how often a positive or negative earnings surprise triggers a regime switch and how long the higher-volatility state persists.\\n\\nKey takeaways (actionable framing given the data constraints)\\n- Quantitative assessment requires daily price data and event dating; the provided material does not deliver realized-vol or GARCH results for AAPL over the last three years.\\n- With the available cues (notably the Q3 revenue up 10% YoY to $94B), one would expect earnings-related news to be a primary driver of short-term volatility, potentially inducing a regime shift around earnings dates, especially if guidance or margins deviate from expectations.\\n- A rigorous three-year analysis would deliver:\\n  - A quantified RV level and its time-varying pattern, including volatility clustering.\\n  - GARCH (and extensions) parameters indicating persistence; and possibly a regime-switching structure that flags low- and high-volatility states.\\n  - Regime-attribution insights showing whether major events (earnings, guidance changes) align with transitions to higher-volatility regimes.\\n- Next steps to produce the requested quantitative summary:\\n  - Acquire daily AAPL price series for the past three years and exact dates of major announcements (earnings, guidance updates).\\n  - Compute RV, fit GARCH-family models, test for persistence, and run a regime-switching specification.\\n  - Tie regime episodes to observed events to identify drivers and risks specific to AAPL (beyond generic macro factors).\\n\\nIf youd like, I can outline a concrete data collection plan or provide a reproducible checklist and code skeleton (e.g., Python/R) to perform the RV and GARCH analyses once you supply the price series and event dates.'}, {'symbol': 'MSFT', 'topic': 'A Microsoft (MSFT)focused volatility analysis over the last three years, examining realized volatility, time-varying conditional volatility via GARCH-type models, and regime-shift dynamics in the stock.', 'summary': 'Note: The provided material does not contain MSFT price or volatility data. The following is a focused mapping of the supplied items to drivers, risks, and regime-level factors that would shape a Microsoft (MSFT)focused volatility analysis over the last three years, with emphasis on realized volatility, time-varying conditional volatility (GARCH-type), and regime shifts.\\n\\n1) Drivers of realized and time-varying (GARCH-type) volatility\\n- AI-enabled product momentum as a volatility driver: The growth of Copilot-related offerings, exemplified by Copilot Studio usage (over 230,000 organizations), suggests more frequent news flow and earnings sensitivity tied to AI adoption, feature releases, and integration success. This can elevate short-horizon realized volatility and sustain higher conditional volatility through shocks to AI monetization expectations.\\n- Revenue-mix and margin sensitivity from AI strategy: The prospect that AI enhancements could alter Office 365 monetization creates a source of earnings risk. Any unforeseen slowdowns or mispricing of AI-driven revenue contribution could produce negative return shocks and a persistent, higher-variance volatility regime, as investors reassess the mix between AI-enabled subscriptions and traditional Office monetization.\\n- Heterogeneous product-cycle effects within a cloud/AI-enabled stack: Shifts in demand for cloud and productivity solutions (e.g., Copilot-related features, agent-building capabilities) can generate episodic volatility around product launches, earnings announcements, and subscription renewals, contributing to volatility clustering characteristic of GARCH processes.\\n\\n2) Regime-level factors and regime-shift dynamics\\n- Two principal regimes likely relevant for MSFT:\\n  - Regime A (AI-accelerating, higher-variance growth regime): Characterized by rapid AI adoption signals, aggressive monetization expectations, and significant dispersion around earnings and product-announcement news. Volatility is elevated but may be mean-reverting as outcomes become clearer.\\n  - Regime B (AI-maturation, lower-variance stability): As AI initiatives mature and revenue impact stabilizes, volatility may lessen with more predictable subscription growth and margins.\\n- Regime transition drivers (event-driven rather than broad macro): Transitions between regimes would be prompted by:\\n  - Major AI product updates, Copilot adoption milestones, or licensing changes impacting Office 365 monetization.\\n  - Earnings surprises or guidance revisions tied to AI revenue contribution versus traditional software licensing.\\n  - Realizations about AI monetization pace (faster-than-expected adoption or slower uptake) altering the markets volatility expectations.\\n- Regime persistence and clustering: Given the typical volatility clustering in tech-related stocks, periods of AI-news-driven shocks are likely to spill over into subsequent periods, forming persistent high-volatility episodes that a Markov-switching GARCH framework could capture.\\n\\n3) Key risks that shape volatility dynamics\\n- AI monetization risk to core Office 365 revenue: The stated risk of a self-defeating economic loop for the core business implies downside earnings risk if AI enhancements do not translate into commensurate incremental monetization, compressing margins and driving negative shocks to MSFT stock.\\n- Execution and adoption risk of Copilot/AI tools: If Copilot-related growth (e.g., Copilot Studio uptake) underwhelms or delays material revenue realization, downside surprises could push MSFT into higher-volatility regimes.\\n- Product-cycle exposure vs. diversification: Heavier reliance on AI-enabled offerings for future growth can amplify sensitivity to news about AI capabilities, integration success, and competitive positioning, compared with more diversified non-AI revenue streams. This can increase both the frequency and magnitude of volatility spikes during transition windows.\\n\\n4) Practical implications for modeling MSFT volatility (conceptual)\\n- Use a GARCH-type framework with exogenous drivers:\\n  - Include AI-related activity signals (e.g., Copilot adoption metrics, AI-feature rollout cadence, public AI monetization milestones) as exogenous inputs to capture shifts in conditional volatility.\\n  - Include Office 365 monetization indicators as a revenue-cycle proxy to reflect potential margin and earnings shocks.\\n- Incorporate regime-switching elements to capture regime dynamics:\\n  - A Markov-switching GARCH or a regime-switching volatility model to distinguish AI-accelerating vs. AI-maturing regimes, with transition probabilities influenced by AI-event announcements and earnings guidance.\\n- Expect asymmetric responses:\\n  - Leverage effects may be present, with negative AI-news surprises causing larger volatility spikes than positive news, consistent with the stated risk of a self-defeating loop compressing core monetization.\\n- Data caveat:\\n  - The source material does not provide MSFT-specific volatility figures or quantitative regime evidence; the above mappings are qualitative in nature and intended to guide model specification and event-study design rather than report empirical results.\\n\\nBottom line\\n- The MSFT-focused volatility profile over the last three years would likely be shaped by AI-driven product momentum (Copilot-related), potential monetization-risk to Office 365, and the timing of AI-related product and earnings news.\\n- Expect volatility to exhibit clustering and potential regime shifts between an AI-accelerating, higher-variance regime and a more stable AI-maturing regime, with transitions tied to AI-event catalysts and earnings outcomes.\\n- For a rigorous analysis, specify exogenous AI/Office monetization drivers in a GARCH model and test for regime-switching behavior around AI-news events, while noting that the provided material offers qualitative signals rather than quantitative volatility metrics.'}]\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# from state import MarketAnalysisState\n",
    "# from entrypoint import initialize_state\n",
    "# from planner import analysis_planner_node\n",
    "# from nodes.market_data_collection import market_data_collection_node\n",
    "\n",
    "\n",
    "graph = StateGraph(MarketAnalysisState)\n",
    "\n",
    "graph.add_node(\"planner\", analysis_planner_node)\n",
    "graph.add_node(\"data\", market_data_collection_node)\n",
    "graph.add_node(\"context\", context_research_node)\n",
    "\n",
    "graph.set_entry_point(\"planner\")\n",
    "graph.add_edge(\"planner\", \"data\")\n",
    "graph.add_edge(\"planner\", \"context\")\n",
    "graph.add_edge(\"data\", END)\n",
    "graph.add_edge(\"context\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "state = initialize_state(\n",
    "    \"Analyze volatility of Apple and Microsoft over the last 3 years\"\n",
    ")\n",
    "\n",
    "final_state = app.invoke(state)\n",
    "\n",
    "print(final_state[\"raw_market_data\"].keys())\n",
    "print(final_state[\"external_context\"] if \"external_context\" in final_state else \"No context collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6b80e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes/data_cleaning.py\n",
    "# from typing import Dict\n",
    "# import pandas as pd\n",
    "\n",
    "# from state import MarketAnalysisState\n",
    "\n",
    "\n",
    "def data_cleaning_node(state: MarketAnalysisState) -> dict:\n",
    "    \"\"\"\n",
    "    Clean and preprocess raw market data.\n",
    "\n",
    "    Produces a canonical dataset per symbol with:\n",
    "    - price (Adj Close preferred, else Close)\n",
    "    - price_raw (Close)\n",
    "    - returns\n",
    "    - normalized price (base 100)\n",
    "    - volume (if available)\n",
    "\n",
    "    Deterministic, metric-agnostic, loop-safe.\n",
    "    \"\"\"\n",
    "\n",
    "    raw_data = state.get(\"raw_market_data\")\n",
    "\n",
    "    if raw_data is None:\n",
    "        raise ValueError(\"raw_market_data is required before cleaning\")\n",
    "\n",
    "    cleaned: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    # ---- Per-symbol cleaning ----\n",
    "    for symbol, df in raw_data.items():\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(f\"Expected DataFrame for {symbol}\")\n",
    "\n",
    "        df = df.copy()\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        # Validate required columns\n",
    "        if \"Close\" not in df.columns:\n",
    "            raise ValueError(f\"'Close' column missing for {symbol}\")\n",
    "\n",
    "        use_adj = \"Adj Close\" in df.columns\n",
    "\n",
    "        # Build canonical frame\n",
    "        out = pd.DataFrame(index=df.index)\n",
    "\n",
    "        out[\"price\"] = df[\"Adj Close\"] if use_adj else df[\"Close\"]\n",
    "        out[\"price_raw\"] = df[\"Close\"]\n",
    "\n",
    "        if \"Volume\" in df.columns:\n",
    "            out[\"volume\"] = df[\"Volume\"]\n",
    "\n",
    "        # Drop rows with missing prices\n",
    "        out = out.dropna(subset=[\"price\"])\n",
    "\n",
    "        # Normalized price (base 100)\n",
    "        out[\"price_normalized\"] = out[\"price\"] / out[\"price\"].iloc[0] * 100\n",
    "\n",
    "        # Returns\n",
    "        out[\"returns\"] = out[\"price\"].pct_change()\n",
    "\n",
    "        # Drop first row introduced by pct_change\n",
    "        out = out.dropna()\n",
    "\n",
    "        cleaned[symbol] = out\n",
    "\n",
    "    # ---- Align time index across all symbols ----\n",
    "    common_index = None\n",
    "    for df in cleaned.values():\n",
    "        common_index = df.index if common_index is None else common_index.intersection(df.index)\n",
    "\n",
    "    for symbol in cleaned:\n",
    "        cleaned[symbol] = cleaned[symbol].loc[common_index]\n",
    "\n",
    "    return {\n",
    "        \"cleaned_market_data\": cleaned\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools/market_metrics.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "# from analysis_plan import Metric\n",
    "\n",
    "TRADING_DAYS = 252\n",
    "\n",
    "\n",
    "def annualize_return(r: float, periods: int) -> float:\n",
    "    return (1 + r) ** (TRADING_DAYS / periods) - 1\n",
    "\n",
    "\n",
    "\n",
    "def compute_total_return(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    r = df[\"price\"].iloc[-1] / df[\"price\"].iloc[0] - 1\n",
    "    return {\n",
    "        \"value\": float(r),\n",
    "        \"sample_size\": len(df)\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_cagr(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    n = len(df)\n",
    "    r = df[\"price\"].iloc[-1] / df[\"price\"].iloc[0] - 1\n",
    "    cagr = annualize_return(r, n)\n",
    "    return {\n",
    "        \"value\": float(cagr),\n",
    "        \"sample_size\": n\n",
    "    }\n",
    "\n",
    "def compute_sharpe(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    returns = df[\"returns\"]\n",
    "    sharpe = np.sqrt(TRADING_DAYS) * returns.mean() / returns.std()\n",
    "    return {\n",
    "        \"value\": float(sharpe),\n",
    "        \"sample_size\": len(returns)\n",
    "    }\n",
    "\n",
    "def compute_max_drawdown(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    cum = df[\"price\"] / df[\"price\"].iloc[0]\n",
    "    peak = cum.cummax()\n",
    "    drawdown = (cum - peak) / peak\n",
    "    return {\n",
    "        \"value\": float(drawdown.min()),\n",
    "        \"sample_size\": len(df)\n",
    "    }\n",
    "\n",
    "def compute_realized_volatility(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    vol = np.sqrt(TRADING_DAYS) * df[\"returns\"].std()\n",
    "    return {\n",
    "        \"value\": float(vol),\n",
    "        \"sample_size\": len(df)\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_rolling_volatility(df: pd.DataFrame, window: int = 30) -> Dict[str, Any]:\n",
    "    rv = np.sqrt(TRADING_DAYS) * df[\"returns\"].rolling(window).std()\n",
    "    return {\n",
    "        \"value\": rv.dropna().to_dict(),\n",
    "        \"window\": window,\n",
    "        \"sample_size\": len(rv.dropna())\n",
    "    }\n",
    "\n",
    "def compute_rolling_correlation(\n",
    "    data: Dict[str, pd.DataFrame],\n",
    "    window: int = 30\n",
    ") -> Dict[str, Any]:\n",
    "    symbols = list(data.keys())\n",
    "    if len(symbols) < 2:\n",
    "        raise ValueError(\"Rolling correlation requires at least two assets\")\n",
    "\n",
    "    s1, s2 = symbols[:2]\n",
    "    r1 = data[s1][\"returns\"]\n",
    "    r2 = data[s2][\"returns\"]\n",
    "\n",
    "    corr = r1.rolling(window).corr(r2)\n",
    "\n",
    "    return {\n",
    "        \"pair\": [s1, s2],\n",
    "        \"value\": corr.dropna().to_dict(),\n",
    "        \"window\": window,\n",
    "        \"sample_size\": len(corr.dropna())\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_beta(\n",
    "    asset_df: pd.DataFrame,\n",
    "    benchmark_df: pd.DataFrame\n",
    ") -> Dict[str, Any]:\n",
    "    r_a = asset_df[\"returns\"]\n",
    "    r_b = benchmark_df[\"returns\"]\n",
    "\n",
    "    cov = np.cov(r_a, r_b)[0, 1]\n",
    "    var = np.var(r_b)\n",
    "\n",
    "    beta = cov / var\n",
    "\n",
    "    return {\n",
    "        \"value\": float(beta),\n",
    "        \"sample_size\": len(r_a)\n",
    "    }\n",
    "\n",
    "def compute_volume(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    if \"volume\" not in df.columns:\n",
    "        return {\"value\": None, \"note\": \"volume not available\"}\n",
    "\n",
    "    return {\n",
    "        \"value\": float(df[\"volume\"].mean()),\n",
    "        \"sample_size\": len(df)\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_amihud(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    if \"volume\" not in df.columns:\n",
    "        return {\"value\": None, \"note\": \"volume not available\"}\n",
    "\n",
    "    illiq = (df[\"returns\"].abs() / df[\"volume\"]).replace([np.inf, -np.inf], np.nan)\n",
    "    return {\n",
    "        \"value\": float(illiq.mean()),\n",
    "        \"sample_size\": len(illiq.dropna())\n",
    "    }\n",
    "\n",
    "def compute_regime_switching(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    # Placeholder: simple volatility thresholding\n",
    "    vol = df[\"returns\"].rolling(30).std()\n",
    "    regimes = (vol > vol.median()).astype(int)\n",
    "\n",
    "    return {\n",
    "        \"regimes\": regimes.dropna().to_dict(),\n",
    "        \"sample_size\": len(regimes.dropna())\n",
    "    }\n",
    "\n",
    "METRIC_FUNCTIONS = {\n",
    "    Metric.TOTAL_RETURN: compute_total_return,\n",
    "    Metric.CAGR: compute_cagr,\n",
    "    Metric.SHARPE_RATIO: compute_sharpe,\n",
    "    Metric.MAX_DRAWDOWN: compute_max_drawdown,\n",
    "    Metric.REALIZED_VOLATILITY: compute_realized_volatility,\n",
    "    Metric.ROLLING_VOLATILITY: compute_rolling_volatility,\n",
    "    Metric.VOLUME: compute_volume,\n",
    "    Metric.AMIHUD_ILLIQUIDITY: compute_amihud,\n",
    "    Metric.REGIME_SWITCHING: compute_regime_switching,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bbcc6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools/compute_market_metrics.py\n",
    "from typing import Dict, Any, List, Optional\n",
    "import pandas as pd\n",
    "# Metric and metric functions are defined elsewhere in the notebook (Metric, METRIC_FUNCTIONS, compute_rolling_correlation, compute_beta)\n",
    "\n",
    "def compute_market_metrics(\n",
    "    cleaned_data: Dict[str, pd.DataFrame],\n",
    "    metrics: List[Metric],\n",
    "    benchmark: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    results: Dict[str, Any] = {}\n",
    "\n",
    "    # Per-asset metrics\n",
    "    for symbol, df in cleaned_data.items():\n",
    "        symbol_results = {}\n",
    "\n",
    "        for metric in metrics:\n",
    "            if metric in METRIC_FUNCTIONS:\n",
    "                symbol_results[metric.value] = METRIC_FUNCTIONS[metric](df)\n",
    "\n",
    "        results[symbol] = symbol_results\n",
    "\n",
    "    # Cross-asset metrics\n",
    "    if Metric.ROLLING_CORRELATION in metrics:\n",
    "        results[\"rolling_correlation\"] = compute_rolling_correlation(cleaned_data)\n",
    "\n",
    "    if Metric.BETA in metrics:\n",
    "        if benchmark is None:\n",
    "            raise ValueError(\"Benchmark required for beta\")\n",
    "\n",
    "        for symbol in cleaned_data:\n",
    "            if symbol == benchmark:\n",
    "                continue\n",
    "\n",
    "            results[symbol][\"beta\"] = compute_beta(\n",
    "                cleaned_data[symbol],\n",
    "                cleaned_data[benchmark]\n",
    "            )\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "261afb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes/statistical_analysis.py\n",
    "\n",
    "# from state import MarketAnalysisState\n",
    "# from tools.compute_market_metrics import compute_market_metrics\n",
    "\n",
    "\n",
    "def statistical_analysis_node(state: MarketAnalysisState) -> dict:\n",
    "    \"\"\"\n",
    "    Computes all requested metrics from cleaned market data.\n",
    "    \"\"\"\n",
    "\n",
    "    plan = state[\"analysis_plan\"]\n",
    "    cleaned_data = state[\"cleaned_market_data\"]\n",
    "\n",
    "    if plan is None or cleaned_data is None:\n",
    "        raise ValueError(\"analysis_plan and cleaned_market_data are required\")\n",
    "\n",
    "    results = compute_market_metrics(\n",
    "        cleaned_data=cleaned_data,\n",
    "        metrics=plan.metrics,\n",
    "        benchmark=plan.universe.benchmark\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"computed_metrics\": results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b4fe4ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['AAPL', 'MSFT', 'rolling_correlation'])\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# from state import MarketAnalysisState\n",
    "# from entrypoint import initialize_state\n",
    "# from planner import analysis_planner_node\n",
    "# from nodes.market_data_collection import market_data_collection_node\n",
    "\n",
    "\n",
    "graph = StateGraph(MarketAnalysisState)\n",
    "\n",
    "graph.add_node(\"planner\", analysis_planner_node)\n",
    "graph.add_node(\"data\", market_data_collection_node)\n",
    "graph.add_node(\"cleaning\", data_cleaning_node)\n",
    "graph.add_node(\"analysis\", statistical_analysis_node)\n",
    "\n",
    "graph.set_entry_point(\"planner\")\n",
    "graph.add_edge(\"planner\", \"data\")\n",
    "graph.add_edge(\"data\", \"cleaning\")\n",
    "graph.add_edge(\"cleaning\", \"analysis\")\n",
    "graph.add_edge(\"analysis\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "state = initialize_state(\n",
    "    \"Analyze volatility of Apple and Microsoft over the last 3 years\"\n",
    ")\n",
    "\n",
    "final_state = app.invoke(state)\n",
    "\n",
    "print(final_state[\"computed_metrics\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fc9a25a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/6xjb_tfs6vdg6llm0_l4vjz00000gn/T/ipykernel_28042/1035011798.py:14: PydanticDeprecatedSince20: `min_items` is deprecated and will be removed, use `min_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  key_findings: List[str] = Field(\n"
     ]
    }
   ],
   "source": [
    "# report_schema.py\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "class ResearchReport(BaseModel):\n",
    "    topic: str = Field(description=\"Research topic\")\n",
    "\n",
    "    executive_summary: str = Field(\n",
    "        min_length=100,\n",
    "        description=\"High-level summary of findings and implications\"\n",
    "    )\n",
    "\n",
    "    key_findings: List[str] = Field(\n",
    "        min_items=1,\n",
    "        description=\"Bullet-point list of key quantitative and qualitative findings\"\n",
    "    )\n",
    "\n",
    "    conclusion: str = Field(\n",
    "        min_length=50,\n",
    "        description=\"Concise concluding assessment\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "59dfc781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes/report_generation.py\n",
    "# from state import MarketAnalysisState\n",
    "# from report_schema import ResearchReport\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "report_llm = llm.with_structured_output(ResearchReport)\n",
    "\n",
    "\n",
    "\n",
    "def report_generation_node(state: MarketAnalysisState) -> dict:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive research report from computed metrics\n",
    "    and external qualitative context.\n",
    "    \"\"\"\n",
    "\n",
    "    plan = state.get(\"analysis_plan\")\n",
    "    computed_metrics = state.get(\"computed_metrics\")\n",
    "    external_context = state.get(\"external_context\", [])\n",
    "\n",
    "    if plan is None or computed_metrics is None:\n",
    "        raise ValueError(\"analysis_plan and computed_metrics are required\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a financial research analyst.\n",
    "\n",
    "Create a comprehensive research report based on the following information.\n",
    "\n",
    "Topic:\n",
    "{plan.topic}\n",
    "\n",
    "Computed quantitative metrics:\n",
    "{computed_metrics}\n",
    "\n",
    "Qualitative external context:\n",
    "{external_context}\n",
    "\n",
    "Instructions:\n",
    "- Base conclusions strictly on the provided data.\n",
    "- Do not invent metrics or facts.\n",
    "- Integrate quantitative results with qualitative context where relevant.\n",
    "- Keep the report concise, analytical, and professional.\n",
    "\n",
    "Generate a well-structured report with:\n",
    "1. Executive Summary\n",
    "2. Key Findings\n",
    "3. Conclusion\n",
    "\"\"\"\n",
    "\n",
    "    report: ResearchReport = report_llm.invoke(prompt)\n",
    "\n",
    "    return {\n",
    "        \"final_report\": report\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d7309",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResearchReport' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 41\u001b[0m\n\u001b[1;32m     35\u001b[0m state \u001b[38;5;241m=\u001b[39m initialize_state(\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyze volatility of Apple and Microsoft over the last 3 years\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m final_state \u001b[38;5;241m=\u001b[39m app\u001b[38;5;241m.\u001b[39minvoke(state)\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfinal_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinal_report\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())\n",
      "File \u001b[0;32m~/Desktop/Market-analysis-ai/venv/lib/python3.9/site-packages/pydantic/main.py:1026\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResearchReport' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# from state import MarketAnalysisState\n",
    "# from entrypoint import initialize_state\n",
    "# from planner import analysis_planner_node\n",
    "# from nodes.market_data_collection import market_data_collection_node\n",
    "\n",
    "graph = StateGraph(MarketAnalysisState)\n",
    "\n",
    "graph.add_node(\"planner\", analysis_planner_node)\n",
    "graph.add_node(\"data\", market_data_collection_node)\n",
    "graph.add_node(\"context\", context_research_node)\n",
    "graph.add_node(\"cleaning\", data_cleaning_node)\n",
    "graph.add_node(\"analysis\", statistical_analysis_node)\n",
    "graph.add_node(\"report\", report_generation_node)\n",
    "\n",
    "# Join node as lambda\n",
    "graph.add_node(\"join\", lambda state: {})\n",
    "\n",
    "graph.set_entry_point(\"planner\")\n",
    "\n",
    "graph.add_edge(\"planner\", \"data\")\n",
    "graph.add_edge(\"planner\", \"context\")\n",
    "\n",
    "graph.add_edge(\"data\", \"join\")\n",
    "graph.add_edge(\"context\", \"join\")\n",
    "graph.add_edge(\"join\", \"cleaning\")\n",
    "\n",
    "graph.add_edge(\"cleaning\", \"analysis\")\n",
    "graph.add_edge(\"analysis\", \"report\")\n",
    "graph.add_edge(\"report\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "state = initialize_state(\n",
    "    \"Analyze volatility of Apple and Microsoft over the last 3 years\"\n",
    ")\n",
    "\n",
    "final_state = app.invoke(state)\n",
    "\n",
    "# return final_state[\"final_report\"].model_dump_json(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "91914027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"topic\": \"Volatility analysis for Apple (AAPL) and Microsoft (MSFT) over the last 3 years\",\n",
      "  \"executive_summary\": \"Over the three-year window, Apple (AAPL) exhibits higher realized volatility than Microsoft (MSFT). AAPLs realized volatility is 0.2550 (25.50%), based on 751 samples, versus MSFTs 0.2364 (23.64%) over the same sample span. For 30-day rolling volatility (window = 30), AAPL generally travels in a lower-to-moderate band (roughly 0.100.32 observed range, with peaks near 0.322) and has a trough around 0.1020.109 in early 2026. MSFTs 30-day rolling volatility shows a pronounced high-volatility episode mid-2023, with values approaching 0.700.72, followed by a reversion to lower-to-moderate levels. The rolling correlation between AAPL and MSFT remains positive and historically high, centering in the 0.60s to high-0.60s range during the early portion of the window and fluctuating over time. Qualitative context from the external material identifies a regime-based view of volatility, particularly for AAPL, with spikes tied to notable company-specific events (product cycles, earnings guidance, supply-chain/regulatory news). For MSFT, the material outlines a framework for direct comparison using GARCH-type models and realized volatility, but formal MSFT-specific metrics are not provided in the supplied data. Overall, the data indicate time-varying volatility regimes for both names, with AAPL showing higher overall volatility and MSFT exhibiting a distinct episodic spike during mid-2023.\",\n",
      "  \"key_findings\": [\n",
      "    \"Realized volatility (annualized): AAPL = 0.2550407 (sample_size = 751) vs MSFT = 0.2364092 (sample_size = 751).\",\n",
      "    \"30-day rolling volatility (window = 30): AAPL generally ranges around 0.200.25 with peaks up to ~0.322 and troughs near ~0.1020.109 (late 2025 to early 2026).\",\n",
      "    \"30-day rolling volatility: MSFT shows a pronounced high-volatility episode around mid-2023, with values near 0.700.72, followed by a reversion to lower-to-moderate levels.\",\n",
      "    \"Rolling correlation (AAPL vs MSFT): generally positive and in the 0.60s range in the early window, with fluctuations over time but a constructive, ongoing linkage between the two stocks volatilities.\",\n",
      "    \"Qualitative context (external summary): AAPL volatility is analyzed through a GARCH-type regime framework highlighting event-driven spikes (product cycles, earnings, supply-chain/regulatory news). The provided material also sketches a framework for MSFT volatility comparison (GARCH-type and realized volatility), though MSFT-specific metrics are not quantified in the supplied data.\"\n",
      "  ],\n",
      "  \"conclusion\": \"In a three-year horizon, AAPL demonstrates higher realized volatility than MSFT (0.255 vs 0.236). The 30-day rolling volatilities reveal MSFTs notable mid-2023 spike, contrasted with AAPLs more persistent, though generally lower, short-term volatility band. The rolling correlation between the two names remains positive and typically elevated, signaling correlated price dynamics, albeit with period-specific deviations. The qualitative context supports a regime-based interpretation: accept periods of elevated volatility tied to company-specific events (AAPL) and episodic spikes in MSFTs volatility related to enterprise/cloud dynamics. Overall, the data portray time-varying volatility regimes for both equities, with AAPL showing higher overall volatility and MSFT exhibiting a pronounced, event-driven volatility spike during mid-2023.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(final_state[\"final_report\"].model_dump_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
